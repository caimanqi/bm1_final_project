{
    "collab_server" : "",
    "contents" : "---\ntitle: \"hw4\"\nauthor: \"Manqi Cai\"\ndate: \"11/28/2017\"\noutput:   \n  html_document\n---\n\n```{r setup, include=FALSE}\nknitr::opts_chunk$set(echo = TRUE)\n```\n\n```{r library package, include=FALSE}\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(janitor)\nlibrary(tidyr)\nlibrary(leaps)\nlibrary(stats)\n\n\n```\n\n\n#Problem 1 (40p)\nFor this problem, you will be using data ‘HeartDisease.csv’. The investigator is mainly interested if there is an association between total cost (in dollars) of patients diagnosed with heart disease and the number of emergency room (ER) visits. Further, the model will need to be adjusted for other factors, including age, gender, number of complications that arose during treatment, and duration of treatment condition.\n\n```{r load data}\nHeartDisease = read.csv(\"HeartDisease.csv\")\n```\n\n\n##1. Provide a short description of the data set: what is the main outcome, main predictor and other important covariates. Also, generate appropriate descriptive statistics for all variables of interest (continuous and categorical) – no tests required. (10p)\n\n```{r}\nnames(HeartDisease)\n\n#descriptive statistics for all continuous variables of interest \nHeartDisease %>% \n  select(-id, -gender, -drugs) %>% \n  summary() %>% \n  knitr::kable()\n```\n\nSummary of the HeartDisease Dataset\n\n*   There are `r nrow(HeartDisease)` observations and `r ncol(HeartDisease)` variables.They are `r names(HeartDisease)`. Where gender and drugs are categorical variables.\n*   Main outcome: Total cost of claims by subscriber.\n*   Main predictor: ERvisits(Emergency room visits.)\n*   Other important covariates: age, gender, complications and duration.\n\n\n\n\n```{r}\n#descriptive statistics for all categorical variables of interest (Gender)\n\nHeartDisease %>% \n  group_by(gender) %>% \n  count()\n```\n\nThere are 608 female and 180 male participants.\n\n```{r}\n#descriptive statistics for all categorical variables of interest (Drugs)\nHeartDisease %>% \n  group_by(drugs) %>% \n  count()\n\n```\n\nThere are 9 kinds of drugs used in the experiment.\n\n##2. Create separate histograms for ‘total cost’ and ‘number of ER visits’. (2p)\n```{r}\n#histograms for ‘total cost’\nHeartDisease %>%\n  ggplot(aes(x = totalcost)) +\n  geom_histogram(color = \"white\", alpha = .5)+\n  ggtitle(\"histogram of total cost\")\n\n#histograms for ‘number of ER visits’.\nHeartDisease %>%\n  ggplot(aes(x = ERvisits)) +\n  geom_histogram(color = \"white\", alpha = .5)+\n  ggtitle(\"histogram of ER visits\")\n\n  \n```\n\na) Because of skewness, you might want to use a transformation of the outcome (e.g., natural logarithm, square root, etc.).  After testing several transformations, chose one, justify it, and use it from this point forward. (2p)\n\n```{r}\n#1.Take logarithm\n#Since we know from the summary part above that there are 0s in total cost, we can not take logarithm directly.\n\nHeartDisease_log = HeartDisease %>%\n  mutate(log_tot = log(totalcost + 0.1))\n\n\nHeartDisease_log %>%\n  ggplot(aes(x = log_tot)) +\n  geom_histogram(color = \"white\", alpha = .5)+\n  ggtitle(\"histogram of logarithm total cost\")\n\n#2. Take Square Root\nHeartDisease_sqrt = HeartDisease %>%\n  mutate(sqrt_tot = sqrt(totalcost))\n\n\nHeartDisease_sqrt %>%\n  ggplot(aes(x = sqrt_tot)) +\n  geom_histogram(color = \"white\", alpha = .5)+\n  ggtitle(\"histogram of logarithm total cost\")  \n```\n\nFrom the two histograms depicted above, I think the logarithm transformation is more fitted for the outcome, since it's more close to normal distribution.\n\n\nb) Dichotomize variable ‘complications’ into: 0 if no complications, and 1 otherwise. (1p)\n\n```{r}\nHeartDisease_log = HeartDisease_log %>% \n  mutate(complications = as.factor(if_else(complications == 0, 0, 1)))\n```\n\n\n##3. Perform a simple linear regression (SLR) of ‘log_tot’ on ‘ERvisits’. This includes a scatterplot and results of the regression, with appropriate comments on significance and interpretation of the slope. (5p)\nNote: all your interpretations should use the original form of the outcome.\n```{r}\n#Scatterplot\nHeartDisease_log %>%\n  ggplot(aes(x = ERvisits, y = log_tot))+\n  geom_point()+\n  geom_smooth(method = \"lm\", se=FALSE, color=\"red\", formula = y ~ x)+\n  ggtitle(\"scatterplot of distribution of ERvisits\")\n\nreg_1 <- lm(log_tot ~ ERvisits, data = HeartDisease_log)\nsummary(reg_1)\n\ntidy(reg_1)\n\nconfint(reg_1)\n\n\n```\n\n\n*  The fitted line is :ln($\\hat{Y}$) = 5.51 + 0.23*$\\hat{X}$\n*    $\\beta_{0}$ = 5.51  $\\beta_{1}$ = 0.23 $\\hat{Y}$ is estimated total cost, $\\hat{X}$ is estimated Emergency room visits.\n*  Significance: The p-value for ERvisits is less than 0.05, and therefore, ERvistis can be consider as a siginificant variable.\n*   Interpretation of $\\beta_{1}$: For every time increase in Emergency room visits, the logarithm of total cost will increase 0.23.\n*   Interpretation of $\\beta_{0}$: When there are no Emergency room visits, the total cost will be $e^{5.51}$ $\\approx$ 247.15 \n\n##4. Fit a multiple linear regression (MLR) with the new binary variable for ‘complications’ and ‘ERvisits’ as predictors.\n\na) Test if binary ‘complications’ is an effect modifier of the relationship between ‘log_tot’ and ‘ERvisits’. Comment. (2p)\n```{r}\nreg_2 = lm(log_tot ~ ERvisits * complications, data = HeartDisease_log)\n\nsummary(reg_2)\n```\nFrom the table given above, we can see that the p-value for the interaction is 0.33>0.05. Therefore, the'complications' is not an effect modifier of the relationship between 'log_tot' and 'ERvisits', since there are no interactions between 'complications' and 'ERvisits'\n\nb) Test if binary ‘complications’ is a confounder of the relationship between ‘log_tot’ and ‘ERvisits’. Comment. (2p)\nNote: look at the estimated coefficient for ‘ERvisits’ with and without binary ‘complications’ in the model. Any changes in magnitude and/or directionality?\n```{r}\nreg_3 = lm(log_tot ~ ERvisits + complications, data = HeartDisease_log)\n\nsummary(reg_3)\nsummary(reg_1)\n```\nFrom the tables given above, we can see that the p-value for adding complivations is <0.05, therefore, we can consider it as a confounder.\n\nThe estimated coefficient for 'ERvisits' with binary 'complications' is 0.20278 and without is 0.22534, therefore, the directionality is not changed, but the magnititude is changed by(0.22534-0.20278)/0.22534 $\\approx$ 0.10\n\nDecide if binary ‘complications’ should be included along with ‘ERvisits. Why or why not? (1p)\n\nBinary 'complications' should be included along with 'ERvisits'. Because the adjusted $R^{2}$ are different between with complications and without complications. What's more, the complications is considered to be significant in the model. Therefore,  binary ‘complications’ should be included along with ‘ERvisits'.\n\nc) Use your choice of model above and add additional covariates (age, gender, and duration of treatment). Fit a MLR, show the regression results, and comment. (5p)\n```{r}\n#add additional covariates (age, gender, and duration of treatment).\nreg_4 = lm(log_tot ~ ERvisits + complications + age + gender + duration, data = HeartDisease_log)\nsummary(reg_4)\n```\nSince the p-value for gender is larger than 0.05, therefore, it's not significant in the model. So we can discard this covariates.\n\nTherefore, the model becomes:\n```{r}\nreg_5 = lm(log_tot ~ ERvisits + complications + age + duration, data = HeartDisease_log)\nsummary(reg_5)\n```\nComments:\n\n*   When we discard the gender covariates, the p-value of all covariates are less than 0.05, and they can be considered as significant.\n*   The fitted line becomes :ln$\\hat{Y_{i}}=5.83 + 0.17* \\hat{X_{i1}} + 1.52 *\\hat{X_{i2}} - 0.02*\\hat{X_{i3}} + 0.006*\\hat{X_{i4}}$\n*   $X_{i1}$ is ERvisits,  $X_{i2}$  is binary complications,  $X_{i3}$ is age,  $X_{i4}$ is duration.\n*   $\\beta_{0}$:5.83  $\\beta_{1}$:0.17  $\\beta_{2}$:1.52  $\\beta_{3}$:- 0.02  $\\beta_{4}$ :0.006\n*   Only age is negative associated with total cost, the rest covariates are positive associated with total cost.\n*   Interpretation of  $\\beta_{1}$: Every unit increased in Emergency room visits, the logarithm of total cost increased by 0.17, while adjusting for other predictors\n*   Interpretation of  $\\beta_{2}$: When there complications, the logarithm of total cost increased by 1.52, while adjusting for other predictors\n*   Interpretation of  $\\beta_{3}$: Every unit increased in age, the logarithm of total cost decreased by 0.02, while adjusting for other predictors\n*   Interpretation of  $\\beta_{4}$: Every unit increased in duration, the logarithm of total cost increased by 0.006, while adjusting for other predictors\n\nd) Using the ‘extended’ model that you chose in part c)\n  i. Assess the model fit. (2.5p)\n```{r}\nsummary(reg_5)\nsummary(reg_1)\n```\nFrom the tables given above, we can see that the adjusted $R^{2}$ changed from original 0.0928 to multiple covariates 0.2582, since the adjusted R-squared increases only if the new term improves the model more than would be expected by chance\n\nii. Use ANOVA to compare it with the SLR model. Interpret. (2.5p)\n\n\n```{r}\n#Anova for MLR(reg_5)\n#Test for each term are conditioned for everything else above it.\nanova(reg_1, reg_5)\n\n```\n$\\because$ \nSmall model: ln($\\hat{Y}$) = 5.51 + 0.23*$\\hat{X}$\nLarge model: ln$\\hat{Y_{i}}=5.83 + 0.17* \\hat{X_{i1}} + 1.52 *\\hat{X_{i2}} - 0.02*\\hat{X_{i3}} + 0.006*\\hat{X_{i4}}$,\n\nFor F-test,\n\n$\\alpha$ = 0.05\n\n$H_{0}$: $\\beta_{2} = \\beta_{3} = \\beta_{4}$\n\n$H_{1}$: $\\beta_{2}\\neq0$ or $\\beta_{3}\\neq0$ or $\\beta_{4}\\neq0$\n\nSince $F^{*}$=59.424 > $F_{3,783,0.95}$=2.62, we can therefore reject the null and conclude that large model is superior.\n\n  \n  \n##5. In a paragraph, summarize your findings to address the primary question posed by the investigator (that has limited statistical knowledge). (5p)\nNote: In your interpretation, you should also provide a point estimate and 95% CI associated to the main predictor of interest.\n```{r}\nconfint(reg_5)\nsummary(reg_5)\n```\nOverall there is association between total cost (in dollars) of patients diagnosed with heart disease and the number of emergency room (ER) visits. We choose to use a multiple linear model to fit the association, which in this case is \n\nln$\\hat{Y_{i}}=5.83 + 0.17* \\hat{X_{i1}} + 1.52 *\\hat{X_{i2}} - 0.02*\\hat{X_{i3}} + 0.006*\\hat{X_{i4}}$,  \n\nwhile $X_{i1}$ is ERvisits,  $X_{i2}$  is binary complications,  $X_{i3}$ is age,  $X_{i4}$ is duration.\n\nInterpretaion of main predictor :\n\n*   The estimate of main predictor falls between (0.124500934,0.215099538) and its point estimate is 0.1698002\n*   Every unit increased in Emergency room visits, the logarithm of total cost increased by 0.17, while adjusting for other predictors\n\n\n#Problem 2 (40p)\nR dataset ‘state.x77’ from library(faraway) contains information on 50 states from 1970s collected by US Census Bureau. The goal is to predict ‘life expectancy’ using a combination of the remaining variables.\n```{r}\nlibrary(faraway)\n\n\n```\n\n##1. Provide a short description of the data set: what is the main outcome, predictors and goal of the analysis. Also, generate appropriate descriptive statistics for all variables of interest (continuous and categorical) – no test required. (3p)\n```{r}\nstate_x77 <- state.x77 %>% \n  as.tibble() %>% \n  clean_names()\n\nnames(state_x77)\nsummary(state_x77)\n```\nShort Description\n\n*   matrix with 50 rows and 8 columns giving the following statistics in the respective columns.\n*   Main outcome:life expectancy(Life Exp)(1969–71)\n*   Goal of the analysis:predict ‘life expectancy’ using a combination of the remaining variables.\n\nPredictors(all continuous):\n\n*   Population:population estimate as of July 1, 1975\n*   Income:per capita income (1974)\n*   Illiteracy:illiteracy (1970, percent of population)\n*   Murder:murder and non-negligent manslaughter rate per 100,000 population (1976)\n*   HS Grad:percent high-school graduates (1970)\n*   Frost:mean number of days with minimum temperature below freezing (1931–1960) in capital or large city\n*   Area:land area in square miles\n\n\n\n##2. Create exploratory plots, e.g., scatter plots, histograms, box-plots to get a sense of the data and possible variable transformations. (2p)\n```{r}\n#Explorations:scatter plot matrix and pairwise correlation\npairs(state_x77)\n#Helps identify collinearity\ncor(state_x77)\n\nattach(state_x77)\n\n#Histograms for each variable \npar(mar=c(3,3,1,1))\npar(mfrow = c(3,3))\nhist(population, main = \"population\")\nhist(income, main = \"income\")\nhist(illiteracy, main = \"illiteracy\")\nhist(life_exp, main = \"life_exp\")\nhist(murder, main = \"murder\")\nhist(hs_grad, main = \"hs_grad\")\nhist(frost, main = \"frost\")\nhist(area, main = \"area\")\n\n#Boxplots for each variable\npar(mar=c(3,3,1,1))\npar(mfrow = c(3,3))\nboxplot(population, main = \"population\")\nboxplot(income, main = \"income\")\nboxplot(illiteracy, main = \"illiteracy\")\nboxplot(life_exp, main = \"life_exp\")\nboxplot(murder, main = \"murder\")\nboxplot(hs_grad, main = \"hs_grad\")\nboxplot(frost, main = \"frost\")\nboxplot(area, main = \"area\")\n\n```\nComments:\n\n*   According to the scatterplot, we can see that  there seems to be linear relationship between life expectancy and income, illiteracy, murder, hs_grad\n*   According the histogram and boxplot, we can see that the distribution of life expectancy is nearly normal, therefore, there is no need to do transformation. However, we can see that for population,illiteracy, and area seems to be skewed, we can consider taking transformation for them.\n```{r}\nstate_x77_log = state_x77 %>% \n  mutate(population_log = log(population),\n         illiteracy_log = log(illiteracy),\n         area_log = log(area)) %>% \n  dplyr::select(-population, -illiteracy,-area)\n\nattach(state_x77_log)\n#Histograms for each variable \npar(mar=c(3,3,1,1))\npar(mfrow = c(3,3))\nhist(population_log, main = \"population_log\")\nhist(income, main = \"income\")\nhist(illiteracy_log, main = \"illiteracy_log\")\nhist(life_exp, main = \"life_exp\")\nhist(murder, main = \"murder\")\nhist(hs_grad, main = \"hs_grad\")\nhist(frost, main = \"frost\")\nhist(area_log, main = \"area_log\")\n\n```\n\n\n##3. Use automatic procedures to find a ‘best subset’ of the full model. Present the results and comment on the following:\n```{r}\n#Fit a regression using all predictors\n\n\nmult_fit <- lm(life_exp ~ ., data = state_x77_log)\nsummary(mult_fit)\n```\n\nBackward Elimination:\n```{r}\n#drop the least t-value covariate, which in this case is area\n#No area\nstep1 <- update(mult_fit, .~ . -area_log)\nsummary(step1)\n\n#take out the next least t-value covariate,which in this case is illiteracy\n#No Illiteracy\nstep2 <- update(step1, .~ . -illiteracy_log)\nsummary(step2)\n\n#take out the next least t-value covariate,which in this case is income\n#No income\nstep3 <- update(step2, .~ . -income)\nsummary(step3)\n\n```\nIn backward elimination, the adjusted $R^{2}$ is 0.7173 in the end, compared to 0.7014 at first, it does not change dramatically, so we eliminate the covariates won't have effects on model building. \n\nThe model we obtain here is \n\n*  life_exp ~ population_log + murder + hs_grad + frost\n```{r}\nmult_fit_backward <- lm(life_exp ~ population_log + murder + hs_grad +frost)\nsummary(mult_fit_backward)\n```\n\n\nForward Elimination\n```{r}\n#Step 1:Fit SLR for all variables. look for the variable with lowest p-value.\nfit1 <- lm(life_exp~population_log)\ntidy(fit1)\n\nfit2 <- lm(life_exp~income)\ntidy(fit2)\n\nfit3 <- lm(life_exp~illiteracy_log)\ntidy(fit3)\n\nfit4 <- lm(life_exp~murder)\ntidy(fit4)\n\nfit5 <- lm(life_exp~hs_grad)\ntidy(fit5)\n\nfit6 <- lm(life_exp~frost)\ntidy(fit6)\n\nfit7 <- lm(life_exp~area_log)\ntidy(fit7)\n\n#Enter first the one with the lowest p-value:murder\nforward1 <- lm(life_exp~murder)\nsummary(forward1)\n\n#Step 2:Enter the one with the lowest p-value in the rest:\nfit1 <- update(forward1, .~. + population_log)\ntidy(fit1)\n\nfit2 <- update(forward1, .~. + income)\ntidy(fit2)\n\nfit3 <- update(forward1, .~. +illiteracy_log)\ntidy(fit3)\n\nfit4 <- update(forward1, .~. +hs_grad)\ntidy(fit4)\n\nfit5 <- update(forward1, .~. + frost)\ntidy(fit5)\n\nfit6 <- update(forward1, .~. + area_log)\ntidy(fit6)\n\n#Enter first the one with the lowest p-value:hs_grad\nforward2 <- update(forward1, .~. +hs_grad)\nsummary(forward2)\n\n#Step 3:Enter the one with the lowest p-value in the rest:\n\nfit1 <- update(forward2, .~. + population_log)\ntidy(fit1)\n\nfit2 <- update(forward2, .~. + income)\ntidy(fit2)\n\nfit3 <- update(forward2, .~. +illiteracy)\ntidy(fit3)\n\nfit4 <- update(forward2, .~. + frost)\ntidy(fit4)\n\nfit5 <- update(forward2, .~. + area_log)\ntidy(fit5)\n\n#Enter first the one with the lowest p-value:frost\nforward3 <- update(forward2, .~. +frost)\nsummary(forward3)\n\n#Step 4:Enter the one with the lowest p-value in the rest:\n\nfit1 <- update(forward3, .~. + population_log)\ntidy(fit1)\n\nfit2 <- update(forward3, .~. + income)\ntidy(fit2)\n\nfit3 <- update(forward3, .~. +illiteracy_log)\ntidy(fit3)\n\nfit4 <- update(forward3, .~. + area_log)\ntidy(fit4)\n\n\n#Enter first the one with the lowest p-value:frost\nforward4 <- update(forward3, .~. +population_log)\nsummary(forward4)\n\n#Step 4:Enter the one with the lowest p-value in the rest:\n\nfit1 <- update(forward4, .~. + income)\ntidy(fit1)\n\nfit2 <- update(forward4, .~. +illiteracy_log)\ntidy(fit2)\n\nfit3 <- update(forward4, .~. + area_log)\ntidy(fit3)\n```\n\np-value of all new added variables are larger than 0.05, which means that they are not siginificant predictor, and we stop here. \n\nThe model we obtained is life_exp ~ murder + hs_grad +frost + population\n```{r}\nmult_fit_forward <- lm(life_exp ~ murder + hs_grad +frost + population_log)\nsummary(mult_fit_forward)\n```\n\nStepwise regression\n```{r}\nstep(mult_fit, direction = \"backward\")\n\nstep(mult_fit, direction = \"both\")\n\n#A more compact way to look at the test-based results\n\nbest <- function(model, ...) \n{\n  subsets <- regsubsets(formula(model), model.frame(model), ...)\n  subsets <- with(summary(subsets),\n                  cbind(p = as.numeric(rownames(which)), which, rss, rsq, adjr2, cp, bic))\n  \n  return(subsets)\n}  \n\n# Select the 'best' 2 models of all subsets\nround(best(mult_fit, nbest = 2), 4)\n\n# Select the 'best' 1 model of all subsets\nround(best(mult_fit, nbest = 1), 4)\n```\nFrom the tables given above, we select the smallest bic value, which is model with 4 predictors with bic -47.1745(population_log, murder, hs_grad, frost).\n\n```{r}\n#Checking the model assumptions for the model:life_exp ~ population +murder + hs_grad +frost \nmult_fit_stepwise <- lm(life_exp ~ population_log + murder + hs_grad +frost)\npar(mfrow = c(2,2))\nplot(mult_fit_stepwise)\n```\n\nThe residuals-fitted plot is distributed around 0, which satisfies the assumption that $E(\\epsilon_{i})=0$\n\na) Do the procedures generate the same model? (3p)\nThe procedures generate the same model:life_exp ~ population +murder + hs_grad +frost \n**$LifeExpectancy = \\beta_{0} +\\beta_{1}population + \\beta_{2}murder +\\beta_{3}hs_grad+\\beta_{4}frost$**\n\n\nb) Is there any variable a close call? What was your decision: keep or discard? Provide arguments for your choice. (Note: this question might have more or less relevance depending on the ‘subset’ you chose). (3p)\n\nNo, there seems to be no close call.\n\n\nc) Is there any association between ‘Illiteracy’ and ‘HS graduation rate’? Does your ‘subset’ contain both? (4p)\n```{r}\nstate_x77 %>% \n  ggplot(aes(x = illiteracy, y = hs_grad))+\n  geom_point()+\n  geom_smooth(method = \"lm\")\n\ncor(illiteracy, hs_grad)\n```\n\nFrom the data given above, we can see that the coefficient between these two variables is -0.6571886, with the scatterplot seems not so linear associated, we may conclude that there seems to be no association between ‘Illiteracy’ and ‘HS graduation rate’.\n\nMy subset does not contain both.\n\n##4. Use criterion-based procedures to guide your selection of the ‘best subset’. Summarize your results (tabular or graphical) (10p).\n```{r}\n# Leaps function provides all-subsets analysis\n\n# Printing the 2 best models of each size, using the Cp criterion:\nleaps(x = state_x77_log[,-2], y = state_x77$life_exp, nbest=1, method=\"Cp\")\n\n\n# Printing the 2 best models of each size, using the adjusted R^2 criterion:\nleaps(x = state_x77_log[,-2], y = state_x77$life_exp, nbest=1, method=\"adjr2\")\n\n# Summary of models for each size (one model per size)\nb<-regsubsets(life_exp ~ ., data=state_x77_log)\n   (rs<-summary(b))\n\n# Plots of Cp and Adj-R2 as functions of parameters\n\npar(mar=c(4,4,1,1))\npar(mfrow=c(1,2))\n\nplot(2:8, rs$cp, xlab=\"No of parameters\", ylab=\"Cp Statistic\")\nabline(0,1)\n\nplot(2:8, rs$adjr2, xlab=\"No of parameters\", ylab=\"Adj R2\")\n\n```\nFrom the 2 plots depicted above, when using Cp criterion.  We found that the when P= 4,the Cp is closest to P. When size = 5, P = 4, it's the minimun Cp.The model fitted this criterion is **$LifeExpectancy = \\beta_{0} +\\beta_{1}murder +\\beta_{2}hs_grad+\\beta_{3}frost$**\n\nWhen using adjusted $R^{2}$criterion, we choose the model with biggest adjusted $R^{2}$,which in this case is P=5.\nThe model is **$LifeExpectancy = \\beta_{0} +\\beta_{1}log(population) + \\beta_{2}murder +\\beta_{3}hs_grad+\\beta_{4}frost$**\n\n##5. Compare the two ‘subsets’ from parts 3 and 4 and recommend a ‘final’ model. Using this ‘final’ model do the following:\n```{r}\n#final model\n#4 predictors(5 parameters)\nfinal_model_4 <- mult_fit_stepwise\nsummary(final_model_4)\n\n#3 predictors(4 parameters)\nfinal_model_3 <- update(mult_fit_stepwise, .~. -population_log)\nsummary(final_model_3)\n\n```\nThe model:\n\n**$LifeExpectancy = \\beta_{0} +\\beta_{1}population_log + \\beta_{2}murder +\\beta_{3}hs_grad+\\beta_{4}frost$** \n\n$\\quad \\quad \\downarrow$\n\n**$LifeExpectancy = 68.72 +0.25log(population) - 0.3murder +0.05hs_grad - 0.005frost$**\n\nThe model:\n\n**$LifeExpectancy = \\beta_{0} +\\beta_{1}murder +\\beta_{2}hs_grad+\\beta_{3}frost$** \n\n$\\quad \\quad \\downarrow$\n\n**$LifeExpectancy = 71.04 -0.28murder +0.05hs_grad - 0.0069frost$**\n\nSince the $R^{2}$ for 4 predictors(5 parameters) and 3 predictors(4 parameters)\nare different，and model with population as predictor has slightly higher $R^{2}$, which means population do have affect the variability of lifeexpectancy.It increase by (0.7173 -0.6939)/0.6939=3%. Based on the procedure before, I still recommend model with population_log as final model.\n\na) Identify any leverage and/or influential points and take appropriate measures. (5p)\n\nOutliers in Y\n```{r}\noutlier_y <- rstandard(final_model_4)\n\noutlier_y = outlier_y[abs(outlier_y) > 2.5]\n\noutlier_y\n```\nThere is no outlier in Y\n\nLeverage Values\n```{r}\ninfluence_measure = as.tibble(influence.measures(final_model_4) $ infmat) %>% \n  mutate(id = 1:nrow(state_x77))\n\n\ninfluence_measure %>% \n  filter(hat > 2*length(final_model_4$coefficients)/nrow(state_x77)) %>% \n    dplyr::select(id, hat)\n\n```\nTherefore, thw 2nd, 5th, 11th, 28th,32th observations have high leverage values.\n\nInfulential points\n```{r}\ninfluence_measure %>% \n  filter(abs(dffit) > 2*sqrt(length(final_model_4$coefficients)/nrow(state_x77))) %>% \n    dplyr::select(id, dffit) \n```\nAccording to DFFIT, the 11th, 47th observations are influential points.\n\n\n```{r}\npar(mfrow = c(2,2))\nplot(final_model_4)\n```\nFrom the residuals vs leverage plot, we can see that 11th observation is close to the 0.5 range\n\nFrom leverage， influential points，and residuals vs leverage plot detect, overall, I think the 11th observations should be taken more consideration, therefore, we can discard the 11th observations.\n```{r}\n#discard 11th observations\nstate_x77_no11 <- state_x77_log %>%\n  mutate(id = 1:nrow(state_x77_log)) %>%\n  filter(id != 11)\n\nrm_lm <- lm(life_exp ~ population_log + murder + hs_grad + frost, data = state_x77_no11)\nsummary(rm_lm)  \nsummary(final_model_4)\n\n\n```\nFrom the summary given above we can see that after discarding the 11th observation, the frost becomes insignificant. Therefore, the 11th observation does have impact on model.\n\nSince the 11th observations contains information about Hawaii, the frost parameter seems to be not so influential. Therefore, I think th 11th observations can be kept in the dataset.\n\nb) Check the model assumptions. (5p)\n```{r}\npar(mfrow = c(2,2))\nplot(final_model_4)\n\n\n```\nThe four assumptions of model is 1. Linearity of residuals 2.Independence of residuals 3.Normal distribution of residuals 4.Equal variance of residuals \n\n\n\n*   1. Linearity of residuals: There seems to be linearity from the residuals vs fitted plot.\n*   2.Independence of residuals: It's satistied since each observation has its own idependent result\n*   3.Normal distribution of residuals:There is a straight line in QQplot(residuals are normal)\n*   4.Equal variance of residuals : Residuals form a horizontal(linear)'band' around zero:above and below (indication of equal variance)\n\n##6. In a paragraph, summarize your findings to address the primary question posed by the investigator (that has limited statistical knowledge). (5p)\nOverall, my final model is \n\n**$LifeExpectancy = \\beta_{0} +\\beta_{1}log(population) + \\beta_{2}murder +\\beta_{3}hs_grad+\\beta_{4}frost$** \n\n$\\quad \\quad \\downarrow$\n\n**$LifeExpectancy = 68.72 +0.25log(population) - 0.3murder +0.05hs_grad - 0.005frost$**\n\nThe population and hs_grad are postive associated with life expectancy, while the murder and frost are negative associuated. However the 11th observation seems to be an outlier in this case\n\n ‘life expectancy’ is a combination of log(population), murder, hs_grad and frost.\n",
    "created" : 1512850287289.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "399555303",
    "id" : "59684DE6",
    "lastKnownWriteTime" : 1512147450,
    "last_content_update" : 1512147450,
    "path" : "~/Documents/BM1/homework/hw4/hw4.Rmd",
    "project_path" : null,
    "properties" : {
    },
    "relative_order" : 5,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_markdown"
}